{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7ba9933",
   "metadata": {},
   "source": [
    "Q1. What is Bayes' theorem?\n",
    "\n",
    "Bayes' theorem, named after the Reverend Thomas Bayes, is a fundamental concept in probability theory and statistics. It describes how to update or revise our beliefs about the probability of an event based on new evidence. The theorem mathematically relates the conditional probability of an event A given event B to the conditional probability of event B given event A.\n",
    "\n",
    "In its simplest form, Bayes' theorem can be stated as:\n",
    "\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "where:\n",
    "- P(A|B) represents the conditional probability of event A given event B.\n",
    "- P(B|A) represents the conditional probability of event B given event A.\n",
    "- P(A) and P(B) are the probabilities of event A and event B, respectively.\n",
    "\n",
    "In other words, Bayes' theorem allows us to calculate the probability of event A occurring given that event B has occurred by incorporating prior knowledge (P(A)) and the likelihood of observing event B given that event A is true (P(B|A)). It provides a way to update our initial beliefs (prior probability) based on new evidence (likelihood) to obtain a revised probability (posterior probability).\n",
    "\n",
    "Bayes' theorem has widespread applications in various fields, including statistics, machine learning, artificial intelligence, medical diagnosis, spam filtering, and more. It provides a framework for making inferences and updating beliefs in the presence of uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f2eba5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb7f2ee9",
   "metadata": {},
   "source": [
    "Q2. What is the formula for Bayes' theorem?\n",
    "\n",
    "Bayes' theorem is a fundamental concept in probability theory and statistics. It describes the relationship between conditional probabilities. The formula for Bayes' theorem is as follows:\n",
    "\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "In this formula:\n",
    "- P(A|B) represents the probability of event A occurring given that event B has already occurred. This is known as the posterior probability.\n",
    "- P(B|A) is the probability of event B occurring given that event A has already occurred. This is known as the likelihood.\n",
    "- P(A) and P(B) are the probabilities of events A and B occurring independently of each other. P(A) is known as the prior probability, and P(B) is the marginal probability.\n",
    "\n",
    "In summary, Bayes' theorem allows us to update our prior beliefs (P(A)) about an event A based on new evidence (event B) by incorporating the likelihood of B given A (P(B|A)) and the marginal probability of B (P(B))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de6df7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c83a6d79",
   "metadata": {},
   "source": [
    "Q3. How is Bayes' theorem used in practice?\n",
    "\n",
    "Bayes' theorem is used in various fields to make probabilistic inferences and update beliefs based on new evidence. Here are a few practical applications of Bayes' theorem:\n",
    "\n",
    "1. Medical Diagnosis: Bayes' theorem is widely used in medical diagnosis. It helps calculate the probability of a disease or condition given certain symptoms, considering the prevalence of the disease in the population and the accuracy of diagnostic tests. By incorporating prior probabilities and test results, Bayes' theorem can help determine the likelihood of a specific diagnosis.\n",
    "\n",
    "2. Spam Filtering: Email spam filters often utilize Bayes' theorem to classify incoming messages as either spam or legitimate. The filter builds a probabilistic model based on previously labeled emails and updates the probabilities of certain words or patterns occurring in spam or non-spam emails. Bayes' theorem is then used to calculate the probability that an incoming email is spam given the presence of specific words or patterns.\n",
    "\n",
    "3. Machine Learning: Bayes' theorem serves as a foundation for various machine learning algorithms, such as Naive Bayes classifiers. These classifiers use Bayes' theorem to estimate the probability of a particular class given observed features. They assume independence between features, simplifying the computations. Naive Bayes classifiers are commonly used in text categorization, sentiment analysis, and spam detection.\n",
    "\n",
    "4. Risk Assessment: Bayes' theorem can be applied in risk assessment and decision-making processes. It helps incorporate prior knowledge and update beliefs based on new information. By considering the prior probabilities of different outcomes and incorporating observed data, Bayes' theorem enables a more accurate assessment of risks and uncertainties.\n",
    "\n",
    "5. A/B Testing: Bayes' theorem is employed in A/B testing, a technique used to compare two or more versions of a webpage, advertisement, or other marketing elements. It helps analyze the results and determine the probability that a certain variation performs better than others based on observed data.\n",
    "\n",
    "These are just a few examples of how Bayes' theorem is used in practice. Its versatility and ability to update probabilities based on new evidence make it a powerful tool in various fields involving uncertainty and probabilistic reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a017fa9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87a35dd4",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between Bayes' theorem and conditional probability?\n",
    "\n",
    "Bayes' theorem is closely related to conditional probability. In fact, Bayes' theorem can be derived from the concept of conditional probability. \n",
    "\n",
    "Conditional probability measures the probability of an event A occurring given that another event B has already occurred. It is denoted as P(A|B), read as \"the probability of A given B.\" Conditional probability is defined as:\n",
    "\n",
    "P(A|B) = P(A ∩ B) / P(B)\n",
    "\n",
    "where P(A ∩ B) represents the probability of both events A and B occurring together, and P(B) is the probability of event B occurring.\n",
    "\n",
    "Bayes' theorem provides a way to reverse the conditional probability by expressing P(A|B) in terms of P(B|A) and the marginal probabilities P(A) and P(B). The formula for Bayes' theorem is:\n",
    "\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "In this formula, P(B|A) represents the probability of event B occurring given that event A has already occurred. By multiplying it with the prior probability P(A) and dividing by the marginal probability P(B), we obtain the posterior probability P(A|B), which is the probability of event A occurring given the occurrence of event B.\n",
    "\n",
    "In summary, Bayes' theorem allows us to calculate the conditional probability of an event A given event B by utilizing the likelihood P(B|A) and the marginal probabilities P(A) and P(B). It provides a way to update our beliefs or probabilities based on new evidence or observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4064a320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f51875ac",
   "metadata": {},
   "source": [
    "Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?\n",
    "\n",
    "When choosing which type of Naive Bayes classifier to use for a given problem, you need to consider the nature of your data and the assumptions made by each variant of the classifier. Here are the three main types of Naive Bayes classifiers and some factors to consider:\n",
    "\n",
    "1. Gaussian Naive Bayes: This classifier assumes that the continuous features in your data follow a Gaussian (normal) distribution. If your features are continuous and approximately Gaussian distributed, Gaussian Naive Bayes can be a suitable choice. It is commonly used for problems involving continuous data, such as numerical measurements.\n",
    "\n",
    "2. Multinomial Naive Bayes: This classifier is appropriate when dealing with discrete features, particularly in the form of counts or frequencies. It is commonly used for text classification tasks, where the features represent word frequencies or presence in a document. Multinomial Naive Bayes assumes that the features are generated from a multinomial distribution.\n",
    "\n",
    "3. Bernoulli Naive Bayes: This classifier is similar to Multinomial Naive Bayes but is specifically designed for binary features. It assumes that features are binary (0 or 1) and follow a Bernoulli distribution. It is often used in text classification tasks where the presence or absence of words in a document is considered.\n",
    "\n",
    "When choosing among these classifiers, you should take into account the nature of your data and the assumptions made by each classifier. Here are some considerations:\n",
    "\n",
    "- Data Type: If your features are continuous, Gaussian Naive Bayes might be suitable. For discrete features, Multinomial or Bernoulli Naive Bayes can be appropriate, depending on whether the features are counts or binary.\n",
    "\n",
    "- Distribution Assumptions: Consider whether the distribution assumptions made by each variant align with your data. Gaussian Naive Bayes assumes a Gaussian distribution, while Multinomial and Bernoulli Naive Bayes assume multinomial and Bernoulli distributions, respectively.\n",
    "\n",
    "- Feature Independence: Naive Bayes classifiers assume feature independence, meaning that the presence or value of one feature does not depend on the presence or value of other features. Assess whether this assumption holds reasonably well for your data. If independence is violated, other classifiers may be more suitable.\n",
    "\n",
    "- Size and Quality of Training Data: The size and quality of your training data also matter. Some variants may perform better than others given certain amounts or quality of data. It's advisable to experiment with different variants and evaluate their performance using appropriate metrics and validation techniques.\n",
    "\n",
    "Overall, the choice of Naive Bayes classifier depends on the specific characteristics of your data, the assumptions made by each variant, and the performance you expect for your problem. It's often beneficial to try different variants and compare their performance to select the most suitable one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781e3a77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f3d7097",
   "metadata": {},
   "source": [
    "Q6. Assignment:\n",
    "You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive\n",
    "Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of\n",
    "each feature value for each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1763f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new instance is predicted to belong to class A.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MD SONU\\AppData\\Local\\Temp\\ipykernel_18388\\4101805780.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  A = np.array([[3, 3, 4], [4, 3, 3, 3]])\n",
      "C:\\Users\\MD SONU\\AppData\\Local\\Temp\\ipykernel_18388\\4101805780.py:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  B = np.array([[2, 2, 1], [2, 2, 2, 3]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the dataset\n",
    "X1 = [1, 2, 3]\n",
    "X2 = [1, 2, 3, 4]\n",
    "A = np.array([[3, 3, 4], [4, 3, 3, 3]])\n",
    "B = np.array([[2, 2, 1], [2, 2, 2, 3]])\n",
    "\n",
    "# Define the new instance\n",
    "new_X1 = 3\n",
    "new_X2 = 4\n",
    "\n",
    "# Calculate likelihood probabilities\n",
    "P_X1_given_A = A[0].count(new_X1) / len(A[0])\n",
    "P_X2_given_A = A[1].count(new_X2) / len(A[1])\n",
    "P_X1_given_B = B[0].count(new_X1) / len(B[0])\n",
    "P_X2_given_B = B[1].count(new_X2) / len(B[1])\n",
    "\n",
    "# Calculate prior probabilities\n",
    "P_A = 0.5\n",
    "P_B = 0.5\n",
    "\n",
    "# Calculate posterior probabilities\n",
    "P_A_given_X = (P_X1_given_A * P_X2_given_A * P_A) / ((P_X1_given_A * P_X2_given_A * P_A) + (P_X1_given_B * P_X2_given_B * P_B))\n",
    "P_B_given_X = (P_X1_given_B * P_X2_given_B * P_B) / ((P_X1_given_A * P_X2_given_A * P_A) + (P_X1_given_B * P_X2_given_B * P_B))\n",
    "\n",
    "# Print the predicted class\n",
    "if P_A_given_X > P_B_given_X:\n",
    "    print(\"The new instance is predicted to belong to class A.\")\n",
    "else:\n",
    "    print(\"The new instance is predicted to belong to class B.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28389d87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

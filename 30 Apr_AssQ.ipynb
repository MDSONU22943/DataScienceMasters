{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef2364a7",
   "metadata": {},
   "source": [
    "Q1. Explain the concept of homogeneity and completeness in clustering evaluation. How are they\n",
    "calculated?\n",
    "\n",
    "In clustering evaluation, homogeneity and completeness are measures used to assess the quality of a clustering result. They provide insights into how well the clustering algorithm groups similar data points together and how well it captures all the data points from the same true class.\n",
    "\n",
    "1. Homogeneity:\n",
    "Homogeneity measures the extent to which each cluster contains only data points that belong to a single class or category. It ensures that elements within a cluster are similar in terms of their true labels. A clustering result is considered highly homogeneous if all clusters consist of data points from the same class.\n",
    "\n",
    "The homogeneity score is calculated using the following formula:\n",
    "Homogeneity = 1 - (H(Y|C) / H(Y))\n",
    "\n",
    "where:\n",
    "- H(Y|C) is the conditional entropy of the true class labels Y given the cluster assignments C.\n",
    "- H(Y) is the entropy of the true class labels.\n",
    "\n",
    "A high homogeneity score (close to 1) indicates that each cluster contains predominantly data points from a single class, suggesting that the clustering result is highly homogeneous.\n",
    "\n",
    "2. Completeness:\n",
    "Completeness measures the extent to which all data points that belong to the same class are assigned to the same cluster. It ensures that elements of the same true class are grouped together. A clustering result is considered highly complete if all data points from the same class are assigned to the same cluster.\n",
    "\n",
    "The completeness score is calculated using the following formula:\n",
    "Completeness = 1 - (H(C|Y) / H(Y))\n",
    "\n",
    "where:\n",
    "- H(C|Y) is the conditional entropy of the cluster assignments C given the true class labels Y.\n",
    "\n",
    "A high completeness score (close to 1) indicates that all data points from the same class are clustered together, suggesting that the clustering result is highly complete.\n",
    "\n",
    "Both homogeneity and completeness scores range from 0 to 1, with 1 indicating the best possible score. It's important to note that a high homogeneity score does not guarantee a high completeness score and vice versa. Therefore, both measures should be considered together to evaluate the clustering result comprehensively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad82b377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1babbb46",
   "metadata": {},
   "source": [
    "Q2. What is the V-measure in clustering evaluation? How is it related to homogeneity and completeness?\n",
    "\n",
    "\n",
    "The V-measure, also known as the V-measure score or the harmonic mean of homogeneity and completeness, is a clustering evaluation metric that combines the concepts of homogeneity and completeness into a single measure. It provides a balanced evaluation of the clustering result by considering both the extent to which clusters contain only data points from the same class (homogeneity) and the extent to which all data points from the same class are grouped together (completeness).\n",
    "\n",
    "The V-measure is calculated using the following formula:\n",
    "\n",
    "V-measure = 2 * (homogeneity * completeness) / (homogeneity + completeness)\n",
    "\n",
    "The V-measure score ranges from 0 to 1, where 1 indicates the best possible score and signifies a clustering result with high homogeneity and completeness.\n",
    "\n",
    "The V-measure takes into account the harmonic mean of homogeneity and completeness, which gives equal weight to both measures. This means that the V-measure penalizes cases where either homogeneity or completeness is low. By using the harmonic mean, the V-measure ensures that both aspects are adequately balanced and considers the worst-case scenario when one of the measures is low.\n",
    "\n",
    "In summary, the V-measure is a single metric that combines both homogeneity and completeness, providing a comprehensive evaluation of the clustering result. It offers a balanced view of how well the clustering algorithm groups similar data points together while capturing all the data points from the same true class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5ba6ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0c950ab",
   "metadata": {},
   "source": [
    "Q3. How is the Silhouette Coefficient used to evaluate the quality of a clustering result? What is the range\n",
    "of its values?\n",
    "\n",
    "The Silhouette Coefficient is a metric used to evaluate the quality of a clustering result by measuring the compactness and separation of the clusters. It provides a measure of how well each data point fits into its assigned cluster and how distinct the clusters are from each other.\n",
    "\n",
    "The Silhouette Coefficient for a single data point is calculated using the following formula:\n",
    "\n",
    "Silhouette Coefficient = (b - a) / max(a, b)\n",
    "\n",
    "where:\n",
    "- \"a\" is the average distance between a data point and other data points within the same cluster (intra-cluster distance).\n",
    "- \"b\" is the average distance between a data point and the data points in the nearest neighboring cluster (inter-cluster distance).\n",
    "\n",
    "The Silhouette Coefficient for a clustering result is then computed as the average of the Silhouette Coefficients of all data points in the dataset.\n",
    "\n",
    "The Silhouette Coefficient ranges from -1 to 1, where:\n",
    "- A value close to 1 indicates that the data points are well-clustered, with each data point being closer to its own cluster compared to other clusters.\n",
    "- A value close to 0 indicates that the data points are on or near the decision boundary between two neighboring clusters.\n",
    "- A negative value indicates that the data points may have been assigned to the wrong clusters, with overlapping or poorly separated clusters.\n",
    "\n",
    "In general, a higher Silhouette Coefficient indicates a better clustering result. However, it is important to note that the Silhouette Coefficient has its limitations. It may not work well with irregularly shaped or overlapping clusters, and it does not account for density-based clusters. Therefore, it is advisable to consider other evaluation metrics and domain-specific knowledge in conjunction with the Silhouette Coefficient to comprehensively assess the clustering quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a24b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9de7275",
   "metadata": {},
   "source": [
    "Q4. How is the Davies-Bouldin Index used to evaluate the quality of a clustering result? What is the range\n",
    "of its values?\n",
    "\n",
    "The Davies-Bouldin Index (DBI) is a clustering evaluation metric that measures the quality of a clustering result based on the inter-cluster and intra-cluster distances. It quantifies the compactness of clusters and the separation between clusters.\n",
    "\n",
    "The DBI for a clustering result is calculated as the average similarity between each cluster and its most similar neighboring cluster. The lower the DBI value, the better the clustering result.\n",
    "\n",
    "The DBI is calculated using the following formula:\n",
    "\n",
    "DBI = (1 / N) * Î£ [max(R(i) + R(j)) / D(i, j)]\n",
    "\n",
    "where:\n",
    "- N is the total number of clusters.\n",
    "- R(i) is the average distance between data point i and all other data points within the same cluster.\n",
    "- D(i, j) is the distance between the centroids of clusters i and j.\n",
    "\n",
    "The DBI measures the ratio of the average distance within clusters to the distance between clusters. A lower DBI value indicates that the clusters are compact and well-separated, with minimal overlap and good distinction.\n",
    "\n",
    "The range of DBI values is from 0 to infinity. The closer the DBI value is to 0, the better the clustering result, indicating well-defined and compact clusters with good separation. Higher DBI values indicate more scattered and overlapping clusters.\n",
    "\n",
    "It is important to note that the DBI has limitations. It assumes that clusters are spherical and equally sized, which may not hold true for all types of data. Additionally, like other evaluation metrics, the DBI should be used in conjunction with other measures and domain-specific knowledge to get a comprehensive understanding of the clustering quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d800810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d94609d1",
   "metadata": {},
   "source": [
    "Q5. Can a clustering result have a high homogeneity but low completeness? Explain with an example.\n",
    "\n",
    "Yes, it is possible for a clustering result to have a high homogeneity but low completeness. This can occur when a clustering algorithm successfully groups data points from the same class together (high homogeneity) but fails to capture all the data points from that class within a single cluster (low completeness).\n",
    "\n",
    "Let's consider an example to illustrate this scenario. Suppose we have a dataset of animals with three classes: dogs, cats, and birds. We apply a clustering algorithm that aims to group animals based on their features. After clustering, we obtain the following result:\n",
    "\n",
    "Cluster 1: Contains all dogs (100% dogs)\n",
    "Cluster 2: Contains some dogs, some cats (50% dogs, 50% cats)\n",
    "Cluster 3: Contains all birds (100% birds)\n",
    "\n",
    "In this example, the clustering result demonstrates high homogeneity because each cluster consists predominantly of a single class. Cluster 1 contains only dogs, Cluster 3 contains only birds, and Cluster 2 contains a mixture of dogs and cats.\n",
    "\n",
    "However, the completeness is low because not all data points from the same class are grouped together. For example, the cats are divided between Cluster 2 and Cluster 3, rather than being grouped into a single cluster. This results in a lower completeness score because not all cats are captured within a single cluster.\n",
    "\n",
    "Therefore, despite high homogeneity (clusters consisting mostly of a single class), the clustering result lacks completeness as it fails to capture all the data points from the same class within a single cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b31b37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d557da23",
   "metadata": {},
   "source": [
    "Q6. How can the V-measure be used to determine the optimal number of clusters in a clustering\n",
    "algorithm?\n",
    "\n",
    "\n",
    "The V-measure can be used as a criterion to determine the optimal number of clusters in a clustering algorithm by comparing the V-measure scores across different numbers of clusters. The idea is to select the number of clusters that maximizes the V-measure, indicating the clustering solution with the best balance of homogeneity and completeness.\n",
    "\n",
    "To determine the optimal number of clusters using the V-measure, you can follow these steps:\n",
    "\n",
    "1. Run the clustering algorithm for different numbers of clusters, ranging from a minimum to a maximum number.\n",
    "\n",
    "2. For each clustering result, calculate the V-measure score.\n",
    "\n",
    "3. Plot the V-measure scores against the corresponding number of clusters.\n",
    "\n",
    "4. Analyze the plot to identify the number of clusters that yields the highest V-measure score.\n",
    "\n",
    "The number of clusters that corresponds to the highest V-measure score can be considered as the optimal number of clusters for the given dataset and clustering algorithm. This indicates the configuration that achieves the best trade-off between homogeneity and completeness.\n",
    "\n",
    "It's worth noting that the choice of the optimal number of clusters using the V-measure should be used in conjunction with other evaluation methods and domain knowledge. It's also important to consider the interpretability and practical implications of the clustering solution in real-world applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc41a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4bcc33e",
   "metadata": {},
   "source": [
    "Q7. What are some advantages and disadvantages of using the Silhouette Coefficient to evaluate a\n",
    "clustering result?\n",
    "\n",
    "\n",
    "Advantages of using the Silhouette Coefficient to evaluate a clustering result:\n",
    "\n",
    "1. Intuitive interpretation: The Silhouette Coefficient provides a measure of how well each data point fits into its assigned cluster and the separation between clusters. It offers an intuitive interpretation of the quality of the clustering result.\n",
    "\n",
    "2. Single metric: The Silhouette Coefficient condenses the evaluation of clustering quality into a single value, making it easy to compare and rank different clustering solutions.\n",
    "\n",
    "3. Considers both compactness and separation: The Silhouette Coefficient takes into account both the intra-cluster and inter-cluster distances, providing a balanced assessment of how well the data points are grouped within clusters and how distinct the clusters are from each other.\n",
    "\n",
    "Disadvantages of using the Silhouette Coefficient to evaluate a clustering result:\n",
    "\n",
    "1. Sensitive to dataset properties: The Silhouette Coefficient can be sensitive to the properties of the dataset, such as the distribution of data points and the shape of clusters. It may not perform well with irregularly shaped or overlapping clusters.\n",
    "\n",
    "2. Assumption of Euclidean distance: The Silhouette Coefficient assumes that the distance measure used is Euclidean. If the data requires a different distance metric, such as in text or categorical data, the Silhouette Coefficient may not be as effective.\n",
    "\n",
    "3. Lack of cluster density consideration: The Silhouette Coefficient does not take into account the density of clusters. It may assign high scores to clusters with different densities, potentially favoring clusters with varying densities.\n",
    "\n",
    "4. Inability to handle noise or outliers: The Silhouette Coefficient does not explicitly handle noise or outliers in the dataset. It treats all data points equally, which can impact the evaluation in the presence of noisy or outlier data.\n",
    "\n",
    "5. Interpretation challenges with negative values: The Silhouette Coefficient can produce negative values when data points are incorrectly assigned to clusters or when clusters overlap significantly. Interpreting negative scores can be challenging and may require further analysis.\n",
    "\n",
    "When using the Silhouette Coefficient, it is important to consider these limitations and complement the evaluation with other metrics and domain knowledge to obtain a comprehensive understanding of the clustering quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798c8a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e70e127",
   "metadata": {},
   "source": [
    "Q8. What are some limitations of the Davies-Bouldin Index as a clustering evaluation metric? How can\n",
    "they be overcome?\n",
    "\n",
    "\n",
    "The Davies-Bouldin Index (DBI) has certain limitations as a clustering evaluation metric:\n",
    "\n",
    "1. Assumes spherical and equally sized clusters: The DBI assumes that clusters are spherical and have equal sizes. However, real-world data often contains clusters of varying shapes and sizes, which may not be accurately captured by the DBI.\n",
    "\n",
    "2. Sensitivity to the number of clusters: The DBI tends to favor clustering solutions with a larger number of clusters. This can lead to an overestimation of the optimal number of clusters if the DBI is used alone.\n",
    "\n",
    "3. Lack of robustness to noise and outliers: The DBI does not explicitly consider the impact of noise or outliers in the dataset. Outliers can significantly affect the distances between clusters and may distort the DBI evaluation.\n",
    "\n",
    "To overcome these limitations, some approaches can be considered:\n",
    "\n",
    "1. Using alternative distance metrics: Instead of relying solely on Euclidean distance, alternative distance metrics more suitable for the dataset can be employed. For example, in text data, measures like cosine similarity may be more appropriate.\n",
    "\n",
    "2. Utilizing other clustering evaluation metrics: It is advisable to use multiple clustering evaluation metrics in conjunction with the DBI. This provides a more comprehensive assessment of the clustering quality and helps overcome the limitations of individual metrics.\n",
    "\n",
    "3. Considering domain knowledge: Incorporating domain knowledge about the data and the problem at hand can provide valuable insights into the clustering quality. Evaluating the clustering result based on the specific requirements and objectives of the application can help overcome the limitations of general-purpose metrics like the DBI.\n",
    "\n",
    "4. Handling outliers and noise: Preprocessing steps, such as outlier detection or noise removal, can be applied to the data before clustering. This helps minimize the influence of outliers on the DBI evaluation.\n",
    "\n",
    "5. Exploratory analysis: Conducting exploratory analysis of the clustering result, visually inspecting the clusters, and considering the interpretability and meaningfulness of the clusters can provide additional insights beyond what the DBI can capture.\n",
    "\n",
    "By considering these strategies and employing a comprehensive evaluation framework, the limitations of the DBI as a clustering evaluation metric can be mitigated, leading to a more robust assessment of clustering quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff009e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b489283",
   "metadata": {},
   "source": [
    "Q9. What is the relationship between homogeneity, completeness, and the V-measure? Can they have\n",
    "different values for the same clustering result?\n",
    "\n",
    "\n",
    "Homogeneity, completeness, and the V-measure are related evaluation metrics for clustering, but they capture different aspects of the clustering result. While homogeneity and completeness focus on specific characteristics of the clustering result, the V-measure combines these measures into a single evaluation metric.\n",
    "\n",
    "Homogeneity measures the extent to which each cluster contains data points from a single class. It quantifies the degree of similarity within clusters. Completeness, on the other hand, measures the extent to which all data points from the same class are grouped together within a single cluster. It captures the extent of capturing all data points from the same class within clusters.\n",
    "\n",
    "The V-measure is a harmonic mean of homogeneity and completeness, providing a balanced evaluation that considers both measures. It assesses the overall quality of the clustering result in terms of both the similarity within clusters and the grouping of data points from the same class.\n",
    "\n",
    "It is possible for homogeneity, completeness, and the V-measure to have different values for the same clustering result. This can occur when the clustering result exhibits different levels of homogeneity and completeness. For example, a clustering result may have high homogeneity, indicating that data points within each cluster predominantly belong to a single class, but at the same time, it may have low completeness, indicating that not all data points from the same class are grouped together within a single cluster.\n",
    "\n",
    "The V-measure takes into account both homogeneity and completeness and provides a comprehensive evaluation that combines these measures. It can help to assess the overall balance between the homogeneity and completeness of the clustering result.\n",
    "\n",
    "In summary, homogeneity and completeness are individual measures that capture specific characteristics of clustering, while the V-measure combines these measures into a single evaluation metric that provides a balanced assessment of the clustering quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48922f34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca756baf",
   "metadata": {},
   "source": [
    "Q10. How can the Silhouette Coefficient be used to compare the quality of different clustering algorithms\n",
    "on the same dataset? What are some potential issues to watch out for?\n",
    "\n",
    "\n",
    "The Silhouette Coefficient can be used to compare the quality of different clustering algorithms on the same dataset by calculating and comparing their respective Silhouette Coefficient scores. Here's how you can use it for comparison:\n",
    "\n",
    "1. Apply multiple clustering algorithms to the same dataset and obtain the clustering results.\n",
    "\n",
    "2. Calculate the Silhouette Coefficient for each clustering result.\n",
    "\n",
    "3. Compare the Silhouette Coefficient scores across the different algorithms. A higher Silhouette Coefficient indicates better clustering quality.\n",
    "\n",
    "When using the Silhouette Coefficient for comparison, it's important to be aware of potential issues:\n",
    "\n",
    "1. Dataset dependency: The Silhouette Coefficient is sensitive to the properties of the dataset, such as its structure, dimensionality, and distribution. Therefore, the comparison should be performed within the context of the specific dataset, and the results may not be directly applicable to other datasets.\n",
    "\n",
    "2. Interpretation challenges: The Silhouette Coefficient provides a relative measure of clustering quality. It can help identify which algorithm performs better among the ones being compared. However, it does not provide an absolute assessment of clustering quality, and its interpretation should be done in conjunction with other evaluation metrics and domain knowledge.\n",
    "\n",
    "3. Limitations with different algorithms: Different clustering algorithms may have their own assumptions and characteristics, which can influence the Silhouette Coefficient scores. For example, algorithms with different distance measures or density-based approaches may yield different results. Therefore, it's important to consider the suitability of the algorithm for the specific dataset and problem domain.\n",
    "\n",
    "4. Consideration of runtime and scalability: The Silhouette Coefficient focuses on evaluating the quality of clustering results but does not consider the computational efficiency or scalability of the algorithms. If runtime or scalability is a concern, it should be taken into account alongside the Silhouette Coefficient when comparing different clustering algorithms.\n",
    "\n",
    "To overcome these potential issues, it is recommended to use the Silhouette Coefficient as one component of a comprehensive evaluation framework. Consider other evaluation metrics, explore the stability of the clustering results, and analyze the interpretability and practical implications of the clustering solutions in the context of the specific dataset and problem domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e3f906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31a265c5",
   "metadata": {},
   "source": [
    "Q11. How does the Davies-Bouldin Index measure the separation and compactness of clusters? What are\n",
    "some assumptions it makes about the data and the clusters?\n",
    "\n",
    "The Davies-Bouldin Index (DBI) measures the separation and compactness of clusters by considering both intra-cluster and inter-cluster distances. It quantifies the balance between the distance within clusters and the distance between clusters to assess the quality of clustering.\n",
    "\n",
    "The DBI is calculated by comparing each cluster with its neighboring clusters. It measures the average dissimilarity between data points within each cluster (intra-cluster distance) and the dissimilarity between the centroids of different clusters (inter-cluster distance).\n",
    "\n",
    "To measure separation, the DBI compares the average dissimilarity within each cluster with the dissimilarity between clusters. A larger inter-cluster distance relative to the intra-cluster distance indicates better separation between clusters.\n",
    "\n",
    "To measure compactness, the DBI evaluates the average dissimilarity within each cluster. A smaller intra-cluster distance indicates a more compact cluster, with data points closer to each other within the cluster.\n",
    "\n",
    "The DBI assumes certain properties and characteristics about the data and the clusters:\n",
    "\n",
    "1. Euclidean distance: The DBI assumes that the distance metric used to calculate dissimilarity is Euclidean. It may not be as effective with datasets that require different distance metrics, such as text or categorical data.\n",
    "\n",
    "2. Spherical clusters: The DBI assumes that the clusters are spherical in shape, meaning they have a similar extent in all dimensions. This assumption may not hold for datasets with irregularly shaped clusters.\n",
    "\n",
    "3. Equal cluster sizes: The DBI assumes that the clusters have equal sizes, meaning they contain a similar number of data points. This assumption may not be valid for datasets with clusters of varying sizes.\n",
    "\n",
    "4. Non-overlapping clusters: The DBI assumes that clusters do not overlap. It does not account for the presence of overlapping clusters in the data.\n",
    "\n",
    "It is important to be mindful of these assumptions when applying the DBI and interpret the results accordingly. Deviations from these assumptions may affect the validity and reliability of the DBI as an evaluation metric for clustering quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c9130d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0225933c",
   "metadata": {},
   "source": [
    "Q12. Can the Silhouette Coefficient be used to evaluate hierarchical clustering algorithms? If so, how?\n",
    "\n",
    "\n",
    "Yes, the Silhouette Coefficient can be used to evaluate hierarchical clustering algorithms. Here's how it can be applied:\n",
    "\n",
    "1. Perform hierarchical clustering on the dataset using the chosen hierarchical clustering algorithm. This algorithm recursively merges or splits clusters to form a hierarchy of clusters.\n",
    "\n",
    "2. Based on the hierarchical clustering result, assign each data point to its corresponding cluster at the desired level of the hierarchy. This can be done by selecting a specific level or cut-off point in the dendrogram.\n",
    "\n",
    "3. Calculate the Silhouette Coefficient for each data point using the assigned clusters. The Silhouette Coefficient measures the cohesion within clusters and the separation between clusters based on the distances between data points.\n",
    "\n",
    "4. Compute the average Silhouette Coefficient across all data points to obtain an overall evaluation score for the hierarchical clustering result.\n",
    "\n",
    "By calculating the Silhouette Coefficient, you can assess the quality and consistency of the clustering solution obtained from the hierarchical clustering algorithm. Higher Silhouette Coefficient values indicate better clustering quality, with well-separated and internally cohesive clusters.\n",
    "\n",
    "It's important to note that when evaluating hierarchical clustering, the choice of the cut-off level in the dendrogram can significantly affect the resulting Silhouette Coefficient. Different cut-off levels can yield different clustering solutions, and therefore, it's advisable to experiment with different cut-off levels and evaluate the Silhouette Coefficient at each level to identify the most appropriate clustering solution.\n",
    "\n",
    "Additionally, it's recommended to compare the Silhouette Coefficients obtained from different hierarchical clustering algorithms or parameter settings to determine the algorithm or configuration that yields the best clustering quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31da22dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44725a72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

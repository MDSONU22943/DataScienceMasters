{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d95217c5",
   "metadata": {},
   "source": [
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
    "represent?\n",
    "\n",
    "In linear regression, R-squared (also known as the coefficient of determination) is a statistical measure used to evaluate the goodness of fit of a regression model. It provides an indication of how well the dependent variable (the variable being predicted) is explained by the independent variables (the predictors) in the model.\n",
    "\n",
    "R-squared is calculated as the proportion of the variance in the dependent variable that can be explained by the independent variables. It ranges from 0 to 1, where 0 indicates that the model does not explain any of the variability in the dependent variable, and 1 indicates that the model explains all the variability.\n",
    "\n",
    "To calculate R-squared, the following steps are typically performed:\n",
    "\n",
    "1. Compute the total sum of squares (SST), which measures the total variability in the dependent variable. It is calculated as the sum of the squared differences between each observed dependent variable value and the mean of the dependent variable.\n",
    "\n",
    "2. Fit the linear regression model and compute the sum of squares of residuals (SSE), which represents the unexplained variability or error in the model. It is calculated as the sum of the squared differences between each observed dependent variable value and the corresponding predicted value from the regression model.\n",
    "\n",
    "3. Calculate the regression sum of squares (SSR), which represents the variability in the dependent variable that is explained by the independent variables. It is calculated as the sum of the squared differences between each predicted dependent variable value and the mean of the dependent variable.\n",
    "\n",
    "4. Finally, R-squared is calculated as SSR divided by SST:\n",
    "\n",
    "   R-squared = SSR / SST\n",
    "\n",
    "R-squared provides a measure of the proportion of the total variation in the dependent variable that is accounted for by the independent variables in the model. It indicates how well the regression model fits the observed data points. A higher R-squared value suggests a better fit, meaning that a larger proportion of the dependent variable's variability is explained by the independent variables. However, R-squared alone cannot determine whether the model is appropriate or whether the predictors are significant. It should be used in conjunction with other statistical measures and considerations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed69c60c",
   "metadata": {},
   "source": [
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n",
    "\n",
    "Adjusted R-squared is a modification of the regular R-squared that accounts for the number of predictors in the linear regression model. While regular R-squared provides a measure of how well the model fits the data, adjusted R-squared adjusts this measure to account for the complexity of the model.\n",
    "\n",
    "Regular R-squared tends to increase as more predictors are added to the model, regardless of whether they have a meaningful impact on the dependent variable. This can lead to overfitting, where the model fits the training data very well but performs poorly on new, unseen data.\n",
    "\n",
    "Adjusted R-squared addresses this issue by penalizing the addition of unnecessary predictors. It takes into account both the goodness of fit and the number of predictors in the model. The formula for adjusted R-squared is:\n",
    "\n",
    "Adjusted R-squared = 1 - [(1 - R-squared) * (n - 1) / (n - p - 1)]\n",
    "\n",
    "where n is the number of observations and p is the number of predictors in the model.\n",
    "\n",
    "The adjusted R-squared value ranges from negative infinity to 1. It penalizes the inclusion of irrelevant predictors, resulting in a lower adjusted R-squared when adding unnecessary variables. Unlike regular R-squared, the adjusted R-squared value can decrease when adding predictors that do not improve the model's fit.\n",
    "\n",
    "By considering the number of predictors, adjusted R-squared provides a more conservative evaluation of the model's explanatory power. It helps to avoid the problem of overfitting by encouraging simplicity and parsimony in the model. Researchers and analysts often prefer adjusted R-squared when comparing different models with different numbers of predictors, as it allows for fairer model comparisons and selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05558a0c",
   "metadata": {},
   "source": [
    "Q3. When is it more appropriate to use adjusted R-squared?\n",
    "\n",
    "Adjusted R-squared is more appropriate to use in situations where you want to compare and evaluate models with different numbers of predictors or when assessing the overall goodness of fit of a model that contains multiple predictors.\n",
    "\n",
    "Here are some scenarios where adjusted R-squared is particularly useful:\n",
    "\n",
    "1. Model comparison: When comparing multiple regression models with varying numbers of predictors, adjusted R-squared helps to account for the complexity of the models. It allows you to assess whether the additional predictors in a more complex model truly contribute to the improvement in the model's fit. Models with higher adjusted R-squared values indicate a better balance between explanatory power and model complexity.\n",
    "\n",
    "2. Variable selection: Adjusted R-squared can aid in variable selection by penalizing the inclusion of unnecessary predictors. It helps to identify models that provide a good fit while keeping the number of predictors to a minimum. Lower adjusted R-squared values for a model with more predictors may indicate that some predictors are not contributing significantly to the model's performance.\n",
    "\n",
    "3. Overfitting detection: Adjusted R-squared is useful for detecting overfitting, which occurs when a model performs well on the training data but poorly on new, unseen data. If the regular R-squared increases with the addition of each predictor, but the adjusted R-squared decreases or remains relatively unchanged, it suggests that the added predictors are not improving the model's explanatory power and might be overfitting the training data.\n",
    "\n",
    "In summary, adjusted R-squared is particularly valuable when comparing models with different numbers of predictors, selecting variables for inclusion in a model, and identifying potential overfitting. It provides a more balanced assessment of model fit by considering both the goodness of fit and the number of predictors, making it a suitable metric in situations where model complexity is a concern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2929002c",
   "metadata": {},
   "source": [
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "calculated, and what do they represent?\n",
    "\n",
    "In the context of regression analysis, RMSE (Root Mean Squared Error), MSE (Mean Squared Error), and MAE (Mean Absolute Error) are commonly used metrics to evaluate the accuracy and performance of regression models. These metrics measure the differences between the predicted values and the actual values of the dependent variable.\n",
    "\n",
    "1. Root Mean Squared Error (RMSE):\n",
    "RMSE is a widely used measure of the average prediction error in regression models. It calculates the square root of the average of the squared differences between the predicted values and the actual values. The formula to calculate RMSE is as follows:\n",
    "\n",
    "RMSE = sqrt( (1/n) * Σ(yi - ŷi)² )\n",
    "\n",
    "where n is the number of observations, yi is the actual value of the dependent variable, and ŷi is the predicted value.\n",
    "\n",
    "RMSE is beneficial because it not only considers the magnitude of the prediction errors but also penalizes larger errors more than MAE. It is measured in the same units as the dependent variable, making it easier to interpret.\n",
    "\n",
    "2. Mean Squared Error (MSE):\n",
    "MSE is another measure of the average prediction error in regression models. It calculates the average of the squared differences between the predicted values and the actual values. The formula for MSE is:\n",
    "\n",
    "MSE = (1/n) * Σ(yi - ŷi)²\n",
    "\n",
    "MSE is closely related to RMSE, but it lacks the square root operation. This means that the MSE value will be in squared units, making it harder to interpret directly in the context of the dependent variable. However, it is commonly used in mathematical calculations and model optimization processes.\n",
    "\n",
    "3. Mean Absolute Error (MAE):\n",
    "MAE is a metric that measures the average magnitude of the prediction errors without considering their direction. It calculates the average of the absolute differences between the predicted values and the actual values. The formula for MAE is:\n",
    "\n",
    "MAE = (1/n) * Σ|yi - ŷi|\n",
    "\n",
    "MAE is the simplest of the three metrics discussed here and provides a straightforward measure of the average prediction error. It is measured in the same units as the dependent variable, making it interpretable and easier to understand.\n",
    "\n",
    "All three metrics, RMSE, MSE, and MAE, provide measures of the prediction accuracy in regression models. Lower values of these metrics indicate better model performance, with the optimal values depending on the specific problem and context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2edf81f",
   "metadata": {},
   "source": [
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "regression analysis.\n",
    "\n",
    "\n",
    "RMSE, MSE, and MAE are commonly used evaluation metrics in regression analysis, each with its own advantages and disadvantages. Let's discuss them:\n",
    "\n",
    "Advantages of RMSE:\n",
    "1. Penalizes large errors: RMSE puts more emphasis on larger errors due to its squared term. This is beneficial in situations where large errors are more critical or need to be given higher weightage.\n",
    "2. Differentiable: RMSE is a differentiable metric, which makes it useful in optimization algorithms that require gradient-based optimization techniques.\n",
    "3. Interpretable: RMSE is measured in the same units as the dependent variable, making it easier to interpret and compare against the scale of the problem.\n",
    "\n",
    "Disadvantages of RMSE:\n",
    "1. Sensitivity to outliers: RMSE is sensitive to outliers because of the squared term. Outliers with large errors can significantly inflate the RMSE value, impacting the overall assessment of the model's performance.\n",
    "2. Bias towards larger errors: The squared term in RMSE amplifies the influence of large errors, which may overemphasize the impact of a few extreme predictions and mask the overall accuracy of the model.\n",
    "3. Lack of direct interpretation: Although RMSE is measured in the same units as the dependent variable, its squared nature makes it harder to interpret directly and compare across different datasets or domains.\n",
    "\n",
    "Advantages of MSE:\n",
    "1. Similar properties to RMSE: MSE shares some advantages with RMSE, such as penalizing large errors and being differentiable.\n",
    "2. Widely used in mathematical calculations: MSE is often used in mathematical derivations and optimization algorithms due to its mathematical properties and ease of computation.\n",
    "\n",
    "Disadvantages of MSE:\n",
    "1. Lack of direct interpretation: Similar to RMSE, MSE is measured in squared units, making it harder to interpret directly in the context of the dependent variable.\n",
    "2. Sensitivity to outliers: MSE is also sensitive to outliers because of the squared term, making it susceptible to the influence of extreme errors.\n",
    "\n",
    "Advantages of MAE:\n",
    "1. Robustness to outliers: MAE is less sensitive to outliers compared to RMSE and MSE because it does not involve squaring the differences. It provides a more balanced evaluation of the model's performance.\n",
    "2. Direct interpretation: MAE is measured in the same units as the dependent variable, making it easy to interpret and compare across different datasets or domains.\n",
    "3. Simplicity: MAE is a simple and straightforward metric to understand and compute.\n",
    "\n",
    "Disadvantages of MAE:\n",
    "1. Equal weighting of errors: MAE treats all errors equally, which means it may not adequately account for the relative importance of different errors. This can be a drawback in situations where certain errors have more significant consequences than others.\n",
    "\n",
    "In summary, the choice of evaluation metric depends on the specific requirements of the problem and the characteristics of the data. RMSE is useful when large errors need to be given more weight, MSE is commonly used in mathematical calculations and optimization algorithms, and MAE provides a more robust and interpretable measure of overall prediction accuracy. It is often recommended to consider multiple metrics and assess them in combination to obtain a comprehensive understanding of the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe9dae8",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
    "it more appropriate to use?\n",
    "\n",
    "Lasso regularization, also known as L1 regularization, is a technique used in linear regression to introduce a penalty term that encourages sparse solutions. It adds a regularization term to the loss function of the regression model, promoting the selection of a subset of the most relevant predictors while forcing others to have zero coefficients.\n",
    "\n",
    "In Lasso regularization, the regularization term is the sum of the absolute values of the coefficients multiplied by a tuning parameter (lambda or alpha). The loss function, with the addition of the regularization term, is optimized to minimize the sum of squared errors and the absolute values of the coefficients simultaneously.\n",
    "\n",
    "The key difference between Lasso regularization and Ridge regularization (L2 regularization) lies in the penalty term. While Lasso uses the absolute values of the coefficients, Ridge uses the squared values. This distinction leads to different effects on the coefficient values and the feature selection process:\n",
    "\n",
    "1. Sparsity: Lasso tends to produce sparse solutions by forcing many coefficients to become exactly zero. This means that Lasso can effectively perform feature selection by identifying the most important predictors and discarding irrelevant ones. In contrast, Ridge does not force coefficients to become zero, but it shrinks them towards zero.\n",
    "\n",
    "2. Subset selection: Lasso's ability to set coefficients to zero makes it suitable for situations where the number of predictors is large compared to the number of observations, or when there is a suspicion that many predictors may be irrelevant or redundant. It can help in identifying a concise set of predictors that have the most impact on the dependent variable.\n",
    "\n",
    "3. Impact on coefficients: Lasso tends to drive some coefficients to zero more aggressively than Ridge, resulting in a more interpretable and parsimonious model. Ridge, on the other hand, does not eliminate any predictors entirely, allowing all predictors to contribute to some extent.\n",
    "\n",
    "4. Multicollinearity handling: Ridge regularization performs better than Lasso when dealing with highly correlated predictors (multicollinearity). Lasso may arbitrarily select one predictor from a group of highly correlated predictors and drive the coefficients of the others to zero. Ridge can handle multicollinearity by shrinking the coefficients towards each other, without completely eliminating them.\n",
    "\n",
    "In summary, Lasso regularization is appropriate when there is a need for sparse solutions, subset selection, or when dealing with situations where there are potentially many irrelevant predictors. It is useful for feature selection and can provide a more interpretable model. However, if multicollinearity is a concern, Ridge regularization may be more suitable. The choice between Lasso and Ridge regularization depends on the specific characteristics of the data, the goal of the analysis, and the trade-off between interpretability and prediction accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efd8241",
   "metadata": {},
   "source": [
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
    "example to illustrate.\n",
    "\n",
    "Regularized linear models, such as Ridge regression and Lasso regression, help prevent overfitting in machine learning by introducing a penalty term that limits the complexity of the model. This penalty term discourages the model from learning intricate patterns in the training data that may not generalize well to new, unseen data.\n",
    "\n",
    "Here's an example to illustrate how regularized linear models prevent overfitting:\n",
    "\n",
    "Consider a dataset with two predictor variables, X1 and X2, and a dependent variable, Y. We want to fit a linear regression model to predict Y based on the predictor variables. The dataset contains 100 observations.\n",
    "\n",
    "Without regularization:\n",
    "If we fit a regular linear regression model without any regularization, it may perfectly fit the training data by finding the best-fitting line that minimizes the sum of squared errors. However, this can lead to overfitting if the model becomes too complex and captures noise or irrelevant features in the data.\n",
    "\n",
    "With regularization:\n",
    "To prevent overfitting, we can apply regularization techniques such as Ridge regression or Lasso regression.\n",
    "\n",
    "- Ridge regression: Ridge regression adds a penalty term to the loss function, which is proportional to the sum of the squared coefficients of the predictor variables. The penalty term is controlled by a tuning parameter (lambda or alpha). As the value of lambda increases, the model's complexity is reduced, and the coefficients are shrunk towards zero. Ridge regression helps to smooth out the coefficients and reduce their magnitudes, leading to a more stable and generalized model.\n",
    "\n",
    "- Lasso regression: Lasso regression also introduces a penalty term to the loss function, but it uses the sum of the absolute values of the coefficients instead. Similar to Ridge regression, Lasso regression encourages coefficient shrinkage. However, Lasso has the additional property of driving some coefficients to exactly zero. This results in a sparse model that automatically performs feature selection by excluding irrelevant predictors.\n",
    "\n",
    "By applying regularization, both Ridge and Lasso regression prevent the model from overfitting by reducing the influence of irrelevant or noisy predictors. They provide a balance between fitting the training data and generalizing well to new data. The regularization techniques help to control the model's complexity and prevent it from memorizing the training data too closely, leading to better performance on unseen data and avoiding overfitting issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd827c1",
   "metadata": {},
   "source": [
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
    "choice for regression analysis.\n",
    "\n",
    "Regularized linear models, such as Ridge regression and Lasso regression, have several limitations that make them not always the best choice for regression analysis in all scenarios. Here are some of their limitations:\n",
    "\n",
    "1. Assumption of linearity: Regularized linear models assume a linear relationship between the predictors and the dependent variable. If the relationship is highly nonlinear, regularized linear models may not capture the underlying patterns accurately and may lead to poor predictions.\n",
    "\n",
    "2. Parameter tuning: Regularized linear models require tuning the regularization parameter (lambda or alpha). Selecting an optimal value for this parameter can be challenging, and different values can significantly impact the model's performance. It often requires cross-validation or other techniques to find an appropriate value, which adds complexity to the modeling process.\n",
    "\n",
    "3. Multicollinearity handling: Regularized linear models, particularly Lasso regression, can struggle with highly correlated predictors (multicollinearity). In the presence of multicollinearity, Lasso regression may arbitrarily select one predictor over the others, leading to instability or biased coefficient estimates. Ridge regression is more robust to multicollinearity, but it does not eliminate the problem entirely.\n",
    "\n",
    "4. Lack of interpretability: Regularized linear models can shrink coefficients towards zero, making the interpretation of individual predictors more challenging. While Ridge regression can still provide non-zero coefficients for all predictors, Lasso regression tends to set some coefficients to exactly zero, effectively excluding those predictors from the model. This can reduce the interpretability of the model, as it may not be clear which predictors are truly important.\n",
    "\n",
    "5. Feature selection limitations: Although Lasso regression performs feature selection by setting some coefficients to zero, it may not always select the \"correct\" set of predictors. In situations where multiple predictors are highly correlated or have similar predictive power, Lasso regression may choose one predictor over another based on chance or small variations in the data. This can lead to an incomplete or suboptimal selection of predictors.\n",
    "\n",
    "6. Data requirements: Regularized linear models may require a relatively large amount of data to achieve stable and reliable results. When the number of observations is limited, regularized linear models may not perform as well and can be prone to overfitting.\n",
    "\n",
    "In summary, while regularized linear models offer valuable regularization techniques to prevent overfitting and control model complexity, they have limitations in handling nonlinearity, multicollinearity, and feature selection. Depending on the specific characteristics of the data and the goals of the analysis, alternative models such as non-linear regression models, tree-based models, or more advanced techniques like ensemble methods or deep learning may be more appropriate choices for regression analysis. It is crucial to consider the specific requirements and limitations of regularized linear models and select the most suitable approach accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498ae865",
   "metadata": {},
   "source": [
    "Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "performer, and why? Are there any limitations to your choice of metric?\n",
    "\n",
    "\n",
    "To determine which model is the better performer, we need to consider the specific context and requirements of the problem. However, based solely on the provided information about the evaluation metrics, we can make some observations:\n",
    "\n",
    "1. RMSE of Model A is 10: RMSE measures the average prediction error, taking into account both the magnitude and direction of errors. A lower RMSE indicates better performance, as it means the model's predictions are, on average, closer to the actual values.\n",
    "\n",
    "2. MAE of Model B is 8: MAE measures the average magnitude of the prediction errors without considering their direction. Similar to RMSE, a lower MAE value indicates better performance, as it means the model's predictions are, on average, closer to the actual values.\n",
    "\n",
    "Considering these observations, we can conclude that Model B performs better than Model A since it has a lower MAE value of 8 compared to the RMSE value of 10 for Model A.\n",
    "\n",
    "However, it's important to note that the choice of evaluation metric is not without limitations. Each metric has its strengths and weaknesses, and the best metric to use depends on the specific context and goals of the problem. Here are a few limitations to consider:\n",
    "\n",
    "1. Sensitivity to outliers: Both RMSE and MAE are sensitive to outliers, but RMSE can be more affected due to the squared term. If there are outliers present in the data, it's essential to examine their impact on the chosen metric.\n",
    "\n",
    "2. Scale of the dependent variable: RMSE and MAE are measured in the units of the dependent variable, making them easier to interpret. However, the choice of metric may be influenced by the specific domain or context of the problem. For example, if the scale of the dependent variable is such that a difference of 10 is considered significant, then an RMSE of 10 might be more acceptable.\n",
    "\n",
    "3. Relative importance of errors: Different evaluation metrics weigh prediction errors differently. RMSE gives more weight to larger errors due to the squared term, while MAE treats all errors equally. The choice of metric should align with the relative importance of different errors in the problem.\n",
    "\n",
    "In summary, based on the provided information, Model B with an MAE of 8 performs better than Model A with an RMSE of 10. However, it is essential to consider the limitations and specific requirements of the problem when selecting the most appropriate evaluation metric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a377bea7",
   "metadata": {},
   "source": [
    "Q10. You are comparing the performance of two regularized linear models using different types of\n",
    "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "method?\n",
    "\n",
    "To determine which regularized linear model performs better, we need to consider the specific context and requirements of the problem. However, based solely on the provided information about the regularization methods and their parameters, we can make some observations:\n",
    "\n",
    "1. Model A uses Ridge regularization with a regularization parameter of 0.1: Ridge regularization introduces a penalty term that is proportional to the sum of the squared coefficients, and the regularization parameter (lambda or alpha) controls the strength of regularization. Smaller values of the regularization parameter result in less shrinkage of the coefficients. A parameter value of 0.1 suggests moderate regularization.\n",
    "\n",
    "2. Model B uses Lasso regularization with a regularization parameter of 0.5: Lasso regularization introduces a penalty term that is proportional to the sum of the absolute values of the coefficients. Similarly, the regularization parameter (lambda or alpha) controls the strength of regularization. A parameter value of 0.5 suggests relatively strong regularization.\n",
    "\n",
    "Considering these observations, we can make some general remarks:\n",
    "\n",
    "- Model A (Ridge regularization with a regularization parameter of 0.1) is likely to have less shrinkage of coefficients compared to Model B (Lasso regularization with a regularization parameter of 0.5). This means that Model A may retain more predictors with non-zero coefficients.\n",
    "\n",
    "- Model B (Lasso regularization) tends to perform feature selection by setting some coefficients exactly to zero. This can be advantageous if there is a suspicion of many irrelevant predictors, as it automatically excludes them from the model. However, the choice of regularization parameter, in this case, is relatively strong (0.5), indicating that Model B may be aggressive in setting coefficients to zero.\n",
    "\n",
    "Ultimately, the decision of which model is better depends on the specific goals and requirements of the problem. Model A (Ridge regularization) might be preferred when it is desirable to retain more predictors and have less aggressive feature selection. Model B (Lasso regularization) might be favored if the goal is to perform feature selection and have a more parsimonious model.\n",
    "\n",
    "It is important to note that the choice of regularization method and its parameter involves trade-offs and limitations:\n",
    "\n",
    "- Ridge regularization tends to shrink coefficients towards zero without eliminating them completely. This can be advantageous in situations with multicollinearity or when retaining more predictors is desired. However, it may not perform as well as Lasso in situations where feature selection is crucial.\n",
    "\n",
    "- Lasso regularization can perform feature selection by setting some coefficients to exactly zero, leading to a sparse model with only the most relevant predictors. However, it may be more sensitive to the choice of the regularization parameter and may not handle multicollinearity as effectively as Ridge regularization.\n",
    "\n",
    "In summary, the choice between Ridge and Lasso regularization depends on the specific goals and requirements of the problem, including the importance of feature selection, the presence of multicollinearity, and the desired complexity of the model. It is essential to consider these factors and assess the trade-offs when selecting the appropriate regularization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d3a402",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7eb783b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44fbf1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0b9671",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
